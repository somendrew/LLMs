{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjR2LwwkpRrk2YIwOMLIre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somendrew/LLMs/blob/main/Self_Multihead_positional_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention ‚Äî Complete Master Notes\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£ Why Self-Attention Exists\n",
        "\n",
        "Traditional sequence models (RNNs, LSTMs):\n",
        "\n",
        "- Process tokens sequentially\n",
        "- Struggle with long-range dependencies\n",
        "- Cannot be fully parallelized\n",
        "- Gradients degrade over long sequences\n",
        "\n",
        "Self-attention solves this by:\n",
        "\n",
        "> Allowing every token to directly attend to every other token in a single operation.\n",
        "\n",
        "Benefits:\n",
        "\n",
        "- Global context access\n",
        "- Parallel computation\n",
        "- Better long-range modeling\n",
        "- Simpler architecture\n",
        "\n",
        "---\n",
        "\n",
        "# 2Ô∏è‚É£ Core Intuition\n",
        "\n",
        "Example:\n",
        "\n",
        "\"The animal didn‚Äôt cross the street because it was too tired.\"\n",
        "\n",
        "To understand \"it\", the model must:\n",
        "\n",
        "- Look at \"animal\"\n",
        "- Compare semantic compatibility\n",
        "- Assign importance\n",
        "\n",
        "Self-attention lets each token compute:\n",
        "\n",
        "> ‚ÄúWhich tokens are relevant to me?‚Äù\n",
        "\n",
        "Each token dynamically builds its own context.\n",
        "\n",
        "---\n",
        "\n",
        "# 3Ô∏è‚É£ Core Components: Query, Key, Value (Q, K, V)\n",
        "\n",
        "Each input embedding is linearly projected into three vectors:\n",
        "\n",
        "- Query (Q) ‚Üí what this token is searching for\n",
        "- Key (K) ‚Üí what this token represents\n",
        "- Value (V) ‚Üí information to pass forward\n",
        "\n",
        "Think:\n",
        "\n",
        "| Component | Analogy |\n",
        "|------------|----------|\n",
        "| Query | Question |\n",
        "| Key | Label |\n",
        "| Value | Content |\n",
        "\n",
        "These are learned projections ‚Äî not manually defined.\n",
        "\n",
        "---\n",
        "\n",
        "# 4Ô∏è‚É£ Mathematical Formulation\n",
        "\n",
        "Let:\n",
        "\n",
        "- Sequence length = N\n",
        "- Model dimension = d_model\n",
        "- Key dimension = d_k\n",
        "\n",
        "Input embeddings:\n",
        "\n",
        "X ‚àà ‚Ñù^(N √ó d_model)\n",
        "\n",
        "Linear projections:\n",
        "\n",
        "Q = XW_Q\n",
        "K = XW_K\n",
        "V = XW_V\n",
        "\n",
        "Where:\n",
        "\n",
        "W_Q, W_K, W_V ‚àà ‚Ñù^(d_model √ó d_k)\n",
        "\n",
        "Now:\n",
        "\n",
        "Q, K, V ‚àà ‚Ñù^(N √ó d_k)\n",
        "\n",
        "---\n",
        "\n",
        "# 5Ô∏è‚É£ Attention Score Matrix\n",
        "\n",
        "Compute similarity:\n",
        "\n",
        "Score = QK·µÄ\n",
        "\n",
        "Shape:\n",
        "\n",
        "(N √ó d_k) ¬∑ (d_k √ó N) = N √ó N\n",
        "\n",
        "Each element (i, j) measures how much token i attends to token j.\n",
        "\n",
        "This produces the attention matrix.\n",
        "\n",
        "---\n",
        "\n",
        "# 6Ô∏è‚É£ Scaled Dot-Product Attention\n",
        "\n",
        "Full formula:\n",
        "\n",
        "Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd_k) V\n",
        "\n",
        "Why divide by ‚àöd_k?\n",
        "\n",
        "Without scaling:\n",
        "\n",
        "- Dot products grow large as dimension increases\n",
        "- Softmax becomes extremely sharp\n",
        "- Gradients vanish\n",
        "\n",
        "Scaling stabilizes variance of dot products.\n",
        "\n",
        "---\n",
        "\n",
        "# 7Ô∏è‚É£ Softmax Step\n",
        "\n",
        "Softmax is applied row-wise.\n",
        "\n",
        "Each row becomes a probability distribution:\n",
        "\n",
        "- Values between 0 and 1\n",
        "- Each row sums to 1\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "For each token:\n",
        "- A distribution over all tokens\n",
        "- Determines importance weights\n",
        "\n",
        "---\n",
        "\n",
        "# 8Ô∏è‚É£ Weighted Sum\n",
        "\n",
        "Final output:\n",
        "\n",
        "Output = Attention_weights ¬∑ V\n",
        "\n",
        "Shape:\n",
        "\n",
        "(N √ó N) ¬∑ (N √ó d_k) = N √ó d_k\n",
        "\n",
        "Each token becomes a weighted mixture of all tokens.\n",
        "\n",
        "This creates contextualized representations.\n",
        "\n",
        "---\n",
        "\n",
        "# 9Ô∏è‚É£ Masking (Critical Detail)\n",
        "\n",
        "Self-attention behaves differently in encoder and decoder.\n",
        "\n",
        "## Encoder Self-Attention\n",
        "- Bidirectional\n",
        "- Tokens can attend to past and future\n",
        "\n",
        "## Decoder Self-Attention\n",
        "- Uses causal mask\n",
        "- Tokens cannot see future tokens\n",
        "\n",
        "Masking sets future positions to -‚àû before softmax.\n",
        "\n",
        "This ensures autoregressive behavior.\n",
        "\n",
        "---\n",
        "\n",
        "# üîü Computational Complexity\n",
        "\n",
        "Self-attention cost:\n",
        "\n",
        "O(N¬≤ ¬∑ d_k)\n",
        "\n",
        "Because attention matrix is N √ó N.\n",
        "\n",
        "This is why very long sequences become expensive.\n",
        "\n",
        "Modern research tries to reduce this (Longformer, FlashAttention, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Tensor Shape Example\n",
        "\n",
        "Example:\n",
        "\n",
        "Batch size = 2  \n",
        "Sequence length = 5  \n",
        "Hidden size = 768  \n",
        "Number of heads = 12  \n",
        "\n",
        "Per head:\n",
        "\n",
        "d_k = 768 / 12 = 64\n",
        "\n",
        "Shapes:\n",
        "\n",
        "Q, K, V ‚Üí (2, 12, 5, 64)\n",
        "\n",
        "Attention matrix ‚Üí (2, 12, 5, 5)\n",
        "\n",
        "Final output ‚Üí (2, 5, 768)\n",
        "\n",
        "Heads are concatenated and projected back to d_model.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ What Self-Attention Is NOT\n",
        "\n",
        "It is not:\n",
        "\n",
        "- A memory lookup\n",
        "- A symbolic reasoning engine\n",
        "- True understanding\n",
        "\n",
        "It is:\n",
        "\n",
        "> Learned statistical weighting of token interactions.\n",
        "\n",
        "Over layers, this produces:\n",
        "\n",
        "- Syntax awareness\n",
        "- Coreference resolution\n",
        "- Semantic similarity\n",
        "- Structural patterns\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ Relationship to Multi-Head Attention\n",
        "\n",
        "Single-head attention learns one type of relationship.\n",
        "\n",
        "Multi-head attention:\n",
        "\n",
        "- Runs attention multiple times in parallel\n",
        "- Each head learns different patterns\n",
        "- Outputs are concatenated\n",
        "\n",
        "Self-attention = core mechanism  \n",
        "Multi-head = parallel enhancement\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ Role in Transformer Layer\n",
        "\n",
        "A full Transformer encoder layer:\n",
        "\n",
        "1. Multi-head self-attention\n",
        "2. Add & LayerNorm\n",
        "3. Feedforward network (MLP)\n",
        "4. Add & LayerNorm\n",
        "\n",
        "Self-attention is only one component of the layer.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£5Ô∏è‚É£ Training vs Inference Behavior\n",
        "\n",
        "Self-attention mechanism itself does NOT change.\n",
        "\n",
        "What changes:\n",
        "\n",
        "- Decoder masking during inference\n",
        "- Autoregressive token feeding\n",
        "\n",
        "But attention computation remains identical.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£6Ô∏è‚É£ Common Interview Questions\n",
        "\n",
        "Q: Why is self-attention better than RNNs?  \n",
        "A: Parallelization + direct long-range dependency modeling.\n",
        "\n",
        "Q: Why scale by ‚àöd_k?  \n",
        "A: Prevents softmax saturation and stabilizes gradients.\n",
        "\n",
        "Q: What is the shape of the attention matrix?  \n",
        "A: N √ó N (per head).\n",
        "\n",
        "Q: Why separate Q, K, V?  \n",
        "A: To decouple similarity computation from information content.\n",
        "\n",
        "Q: What is the computational bottleneck?  \n",
        "A: O(N¬≤) memory and compute.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£7Ô∏è‚É£ One-Line Summary\n",
        "\n",
        "Self-attention allows each token to dynamically compute a weighted combination of all other tokens using learned similarity projections, producing contextualized representations.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£8Ô∏è‚É£ Ultra-Compressed Formula View\n",
        "\n",
        "Given X:\n",
        "\n",
        "Q = XW_Q  \n",
        "K = XW_K  \n",
        "V = XW_V  \n",
        "\n",
        "Attention = softmax(QK·µÄ / ‚àöd_k) V\n",
        "\n",
        "That is the entire mechanism.\n"
      ],
      "metadata": {
        "id": "lHo6k9UPvRsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention ‚Äî Complete Master Notes\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£ Why Multi-Head Attention Exists\n",
        "\n",
        "Single-head attention can only learn **one type of relationship** at a time.\n",
        "\n",
        "Example relationships:\n",
        "- Subject‚Äìverb agreement\n",
        "- Coreference resolution\n",
        "- Long-range dependency\n",
        "- Positional patterns\n",
        "- Semantic similarity\n",
        "\n",
        "Instead of forcing one attention mechanism to capture everything,\n",
        "Transformers use **multiple attention heads in parallel**.\n",
        "\n",
        "> Each head learns a different representation subspace.\n",
        "\n",
        "This increases expressiveness.\n",
        "\n",
        "---\n",
        "\n",
        "# 2Ô∏è‚É£ Core Idea\n",
        "\n",
        "Instead of computing:\n",
        "\n",
        "Attention(Q, K, V)\n",
        "\n",
        "Once,\n",
        "\n",
        "We compute it **h times in parallel**.\n",
        "\n",
        "Each head:\n",
        "\n",
        "- Has its own W_Q, W_K, W_V\n",
        "- Works in a smaller dimensional space\n",
        "- Learns different patterns\n",
        "\n",
        "Then we:\n",
        "\n",
        "1. Concatenate outputs of all heads\n",
        "2. Apply a final linear projection\n",
        "\n",
        "---\n",
        "\n",
        "# 3Ô∏è‚É£ Mathematical Formulation\n",
        "\n",
        "Let:\n",
        "\n",
        "- d_model = model dimension\n",
        "- h = number of heads\n",
        "- d_k = d_model / h\n",
        "\n",
        "Input:\n",
        "\n",
        "X ‚àà ‚Ñù^(N √ó d_model)\n",
        "\n",
        "For each head i:\n",
        "\n",
        "Q_i = XW_Q_i  \n",
        "K_i = XW_K_i  \n",
        "V_i = XW_V_i  \n",
        "\n",
        "Where:\n",
        "\n",
        "W_Q_i, W_K_i, W_V_i ‚àà ‚Ñù^(d_model √ó d_k)\n",
        "\n",
        "Compute attention per head:\n",
        "\n",
        "head_i = softmax(Q_i K_i·µÄ / ‚àöd_k) V_i\n",
        "\n",
        "Each head output shape:\n",
        "\n",
        "‚Ñù^(N √ó d_k)\n",
        "\n",
        "---\n",
        "\n",
        "# 4Ô∏è‚É£ Concatenation Step\n",
        "\n",
        "After computing all heads:\n",
        "\n",
        "Concat(head‚ÇÅ, head‚ÇÇ, ..., head_h)\n",
        "\n",
        "Shape:\n",
        "\n",
        "‚Ñù^(N √ó (h √ó d_k))\n",
        "\n",
        "Since:\n",
        "\n",
        "h √ó d_k = d_model\n",
        "\n",
        "So concatenated output shape:\n",
        "\n",
        "‚Ñù^(N √ó d_model)\n",
        "\n",
        "---\n",
        "\n",
        "# 5Ô∏è‚É£ Final Linear Projection\n",
        "\n",
        "Apply output projection:\n",
        "\n",
        "Output = Concat(...) W_O\n",
        "\n",
        "Where:\n",
        "\n",
        "W_O ‚àà ‚Ñù^(d_model √ó d_model)\n",
        "\n",
        "This mixes information from all heads.\n",
        "\n",
        "Final output shape:\n",
        "\n",
        "‚Ñù^(N √ó d_model)\n",
        "\n",
        "---\n",
        "\n",
        "# 6Ô∏è‚É£ Full Formula\n",
        "\n",
        "MultiHead(Q, K, V) = Concat(head‚ÇÅ, ..., head_h) W_O\n",
        "\n",
        "Where:\n",
        "\n",
        "head_i = Attention(QW_Q_i, KW_K_i, VW_V_i)\n",
        "\n",
        "---\n",
        "\n",
        "# 7Ô∏è‚É£ Why Split Into Smaller Dimensions?\n",
        "\n",
        "If we kept full dimension per head:\n",
        "\n",
        "Cost would explode.\n",
        "\n",
        "Instead:\n",
        "\n",
        "d_k = d_model / h\n",
        "\n",
        "So total computation stays similar to single-head attention.\n",
        "\n",
        "Benefits:\n",
        "\n",
        "- Multiple representation subspaces\n",
        "- No increase in output dimension\n",
        "- Better learning capacity\n",
        "\n",
        "---\n",
        "\n",
        "# 8Ô∏è‚É£ Intuition Example\n",
        "\n",
        "Imagine 8 heads:\n",
        "\n",
        "Head 1 ‚Üí grammar relationships  \n",
        "Head 2 ‚Üí semantic similarity  \n",
        "Head 3 ‚Üí positional relations  \n",
        "Head 4 ‚Üí coreference  \n",
        "Head 5 ‚Üí long-range dependency  \n",
        "Head 6 ‚Üí local context  \n",
        "Head 7 ‚Üí phrase boundaries  \n",
        "Head 8 ‚Üí global sentence meaning  \n",
        "\n",
        "Each head sees the same sentence,\n",
        "but focuses on different aspects.\n",
        "\n",
        "---\n",
        "\n",
        "# 9Ô∏è‚É£ Tensor Shape Example (Concrete)\n",
        "\n",
        "Example:\n",
        "\n",
        "Batch size = 2  \n",
        "Sequence length = 5  \n",
        "d_model = 768  \n",
        "h = 12  \n",
        "\n",
        "Then:\n",
        "\n",
        "d_k = 768 / 12 = 64\n",
        "\n",
        "After projection:\n",
        "\n",
        "Q, K, V ‚Üí (2, 5, 768)\n",
        "\n",
        "Reshaped per head:\n",
        "\n",
        "(2, 12, 5, 64)\n",
        "\n",
        "Attention matrix per head:\n",
        "\n",
        "(2, 12, 5, 5)\n",
        "\n",
        "Output per head:\n",
        "\n",
        "(2, 12, 5, 64)\n",
        "\n",
        "After concatenation:\n",
        "\n",
        "(2, 5, 768)\n",
        "\n",
        "After W_O:\n",
        "\n",
        "(2, 5, 768)\n",
        "\n",
        "---\n",
        "\n",
        "# üîü Computational Complexity\n",
        "\n",
        "Still dominated by:\n",
        "\n",
        "O(N¬≤ ¬∑ d_model)\n",
        "\n",
        "Because each head computes N √ó N attention.\n",
        "\n",
        "Multi-head does NOT change quadratic complexity.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Encoder vs Decoder Multi-Head Attention\n",
        "\n",
        "In Encoder:\n",
        "\n",
        "- Multi-head self-attention\n",
        "- Bidirectional\n",
        "\n",
        "In Decoder:\n",
        "\n",
        "Two types of multi-head attention:\n",
        "\n",
        "1. Masked self-attention\n",
        "2. Cross-attention (attends to encoder output)\n",
        "\n",
        "Cross-attention uses:\n",
        "\n",
        "Queries ‚Üí from decoder  \n",
        "Keys/Values ‚Üí from encoder  \n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Why It Works So Well\n",
        "\n",
        "Because:\n",
        "\n",
        "‚úî Parallel attention mechanisms  \n",
        "‚úî Different learned projection spaces  \n",
        "‚úî Richer feature extraction  \n",
        "‚úî Improves gradient flow  \n",
        "‚úî Enables specialization  \n",
        "\n",
        "It increases model capacity without increasing output dimension.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ What Multi-Head Attention Is NOT\n",
        "\n",
        "It is not:\n",
        "\n",
        "- Multiple independent models\n",
        "- Multiple independent sequences\n",
        "\n",
        "All heads share the same input,\n",
        "but learn different projections.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ Implementation Insight (PyTorch Style)\n",
        "\n",
        "In practice:\n",
        "\n",
        "Instead of storing separate matrices for each head,\n",
        "frameworks often:\n",
        "\n",
        "- Use one big W_Q, W_K, W_V\n",
        "- Then reshape into heads\n",
        "\n",
        "Example shape:\n",
        "\n",
        "W_Q ‚àà ‚Ñù^(d_model √ó d_model)\n",
        "\n",
        "Then split into h chunks.\n",
        "\n",
        "More efficient.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£5Ô∏è‚É£ Common Interview Questions\n",
        "\n",
        "Q: Why use multiple heads instead of one large head?  \n",
        "A: Allows learning diverse relationships in different subspaces.\n",
        "\n",
        "Q: Does multi-head increase output size?  \n",
        "A: No. After concatenation and projection, output size = d_model.\n",
        "\n",
        "Q: Does it increase computational complexity?  \n",
        "A: Still O(N¬≤), but more expressive.\n",
        "\n",
        "Q: Why must d_model be divisible by number of heads?  \n",
        "A: Because we split hidden dimension evenly across heads.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£6Ô∏è‚É£ Key Insight\n",
        "\n",
        "Self-attention learns relationships.\n",
        "\n",
        "Multi-head attention learns multiple types of relationships simultaneously.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£7Ô∏è‚É£ One-Line Summary\n",
        "\n",
        "Multi-head attention runs multiple scaled dot-product attention mechanisms in parallel, concatenates their outputs, and projects them back to the original dimension to increase representational power.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£8Ô∏è‚É£ Ultra-Compressed Formula View\n",
        "\n",
        "For head i:\n",
        "\n",
        "head_i = softmax((XW_Q_i)(XW_K_i)·µÄ / ‚àöd_k)(XW_V_i)\n",
        "\n",
        "MultiHead(X) = Concat(head‚ÇÅ, ..., head_h) W_O\n"
      ],
      "metadata": {
        "id": "Lbe828Ylw_pN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding ‚Äî Complete Master Notes\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£ Why Positional Encoding Is Needed\n",
        "\n",
        "Self-attention has **no built-in notion of order**.\n",
        "\n",
        "It treats input as a set, not a sequence.\n",
        "\n",
        "Example:\n",
        "\n",
        "\"dog bites man\"\n",
        "\"man bites dog\"\n",
        "\n",
        "Without positional information,\n",
        "self-attention would treat both as identical collections of tokens.\n",
        "\n",
        "Therefore:\n",
        "\n",
        "> We must inject position information into token embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "# 2Ô∏è‚É£ Where Positional Encoding Is Applied\n",
        "\n",
        "Before entering the Transformer layers:\n",
        "\n",
        "Final input to model:\n",
        "\n",
        "Input_Embedding = Token_Embedding + Positional_Encoding\n",
        "\n",
        "Both have shape:\n",
        "\n",
        "‚Ñù^(N √ó d_model)\n",
        "\n",
        "Where:\n",
        "\n",
        "N = sequence length  \n",
        "d_model = hidden dimension  \n",
        "\n",
        "---\n",
        "\n",
        "# 3Ô∏è‚É£ Two Main Types of Positional Encoding\n",
        "\n",
        "There are two primary approaches:\n",
        "\n",
        "1Ô∏è‚É£ Fixed (Sinusoidal) Positional Encoding  \n",
        "2Ô∏è‚É£ Learned Positional Embeddings  \n",
        "\n",
        "---\n",
        "\n",
        "# 4Ô∏è‚É£ Sinusoidal Positional Encoding (Original Transformer)\n",
        "\n",
        "Introduced in:\n",
        "\n",
        "\"Attention Is All You Need\"\n",
        "\n",
        "It uses sine and cosine functions of different frequencies.\n",
        "\n",
        "Formula:\n",
        "\n",
        "For position pos and dimension i:\n",
        "\n",
        "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))  \n",
        "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "\n",
        "Properties:\n",
        "\n",
        "- Even dimensions use sine\n",
        "- Odd dimensions use cosine\n",
        "- Different frequencies per dimension\n",
        "\n",
        "---\n",
        "\n",
        "# 5Ô∏è‚É£ Why Sine and Cosine?\n",
        "\n",
        "Important properties:\n",
        "\n",
        "‚úî Unique encoding for each position  \n",
        "‚úî Allows model to learn relative positions  \n",
        "‚úî Periodic structure  \n",
        "‚úî Can generalize to longer sequences than seen in training  \n",
        "\n",
        "Because:\n",
        "\n",
        "sin(a + b) can be expressed using sin(a), cos(a)\n",
        "\n",
        "So the model can infer relative distance.\n",
        "\n",
        "---\n",
        "\n",
        "# 6Ô∏è‚É£ Shape Example\n",
        "\n",
        "Example:\n",
        "\n",
        "Sequence length = 5  \n",
        "d_model = 8  \n",
        "\n",
        "Positional Encoding matrix shape:\n",
        "\n",
        "(5 √ó 8)\n",
        "\n",
        "Each row corresponds to a position.\n",
        "\n",
        "Example (conceptual):\n",
        "\n",
        "Position 0 ‚Üí [0, 1, 0, 1, 0, 1, 0, 1]  \n",
        "Position 1 ‚Üí [sin(1), cos(1), sin(freq), cos(freq), ...]  \n",
        "Position 2 ‚Üí different values  \n",
        "\n",
        "---\n",
        "\n",
        "# 7Ô∏è‚É£ Learned Positional Embeddings\n",
        "\n",
        "Instead of fixed sinusoids:\n",
        "\n",
        "We create a learnable embedding matrix:\n",
        "\n",
        "P ‚àà ‚Ñù^(max_seq_length √ó d_model)\n",
        "\n",
        "Each position has its own trainable vector.\n",
        "\n",
        "Used in:\n",
        "\n",
        "- GPT models\n",
        "- BERT\n",
        "- Most modern LLMs\n",
        "\n",
        "Advantages:\n",
        "\n",
        "‚úî More flexible  \n",
        "‚úî Learns task-specific position patterns  \n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "‚úñ Cannot extrapolate beyond trained sequence length  \n",
        "\n",
        "---\n",
        "\n",
        "# 8Ô∏è‚É£ Absolute vs Relative Positional Encoding\n",
        "\n",
        "## Absolute Position Encoding\n",
        "Encodes:\n",
        "\n",
        "\"This token is at position 5.\"\n",
        "\n",
        "Used in:\n",
        "- Original Transformer\n",
        "- BERT\n",
        "- GPT-2\n",
        "\n",
        "---\n",
        "\n",
        "## Relative Position Encoding\n",
        "\n",
        "Encodes:\n",
        "\n",
        "\"This token is 3 positions away from another token.\"\n",
        "\n",
        "Used in:\n",
        "- Transformer-XL\n",
        "- T5\n",
        "- Modern architectures\n",
        "\n",
        "More powerful for long sequences.\n",
        "\n",
        "---\n",
        "\n",
        "# 9Ô∏è‚É£ Why Addition (Not Concatenation)?\n",
        "\n",
        "We add positional encoding instead of concatenating.\n",
        "\n",
        "Why?\n",
        "\n",
        "If concatenated:\n",
        "\n",
        "Dimension would double ‚Üí computational cost increases.\n",
        "\n",
        "By adding:\n",
        "\n",
        "- Keeps dimension = d_model\n",
        "- Forces model to integrate position into representation\n",
        "\n",
        "---\n",
        "\n",
        "# üîü Important Insight\n",
        "\n",
        "Token embedding captures:\n",
        "\n",
        "\"What this word means.\"\n",
        "\n",
        "Positional encoding captures:\n",
        "\n",
        "\"Where this word is.\"\n",
        "\n",
        "The sum provides:\n",
        "\n",
        "\"What this word means at this position.\"\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ How Position Information Propagates\n",
        "\n",
        "After addition:\n",
        "\n",
        "Multi-head attention processes position-aware embeddings.\n",
        "\n",
        "Through layers:\n",
        "\n",
        "- Relative distances become encoded\n",
        "- Structural information emerges\n",
        "- Word order relationships are learned\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ What Happens Without Positional Encoding?\n",
        "\n",
        "Model becomes permutation invariant.\n",
        "\n",
        "Meaning:\n",
        "\n",
        "Reordering tokens gives same result.\n",
        "\n",
        "This would break language modeling.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ Computational Cost\n",
        "\n",
        "Positional encoding:\n",
        "\n",
        "- Very cheap\n",
        "- O(N √ó d_model)\n",
        "\n",
        "Compared to attention:\n",
        "\n",
        "- O(N¬≤ √ó d_model)\n",
        "\n",
        "Position encoding is negligible in cost.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ Modern Variants\n",
        "\n",
        "Modern large models often use:\n",
        "\n",
        "- Rotary Positional Embeddings (RoPE)\n",
        "- ALiBi (Attention with Linear Biases)\n",
        "- Relative bias encodings\n",
        "\n",
        "These improve:\n",
        "\n",
        "‚úî Long context performance  \n",
        "‚úî Extrapolation  \n",
        "‚úî Stability  \n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£5Ô∏è‚É£ Example: GPT vs BERT\n",
        "\n",
        "BERT:\n",
        "- Learned absolute position embeddings\n",
        "- Encoder-only\n",
        "- Bidirectional\n",
        "\n",
        "GPT:\n",
        "- Learned absolute position embeddings (earlier versions)\n",
        "- Decoder-only\n",
        "- Causal mask applied\n",
        "\n",
        "Modern GPT variants use improved position schemes.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£6Ô∏è‚É£ Common Interview Questions\n",
        "\n",
        "Q: Why do we need positional encoding?  \n",
        "A: Self-attention has no inherent order awareness.\n",
        "\n",
        "Q: Why sine and cosine?  \n",
        "A: Enables relative position inference and generalization.\n",
        "\n",
        "Q: Why add instead of concatenate?  \n",
        "A: Keeps dimension fixed and computationally efficient.\n",
        "\n",
        "Q: What is the difference between absolute and relative encoding?  \n",
        "A: Absolute encodes index; relative encodes distance between tokens.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£7Ô∏è‚É£ One-Line Summary\n",
        "\n",
        "Positional encoding injects order information into token embeddings so that self-attention can model sequence structure.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£8Ô∏è‚É£ Ultra-Compressed View\n",
        "\n",
        "Input = TokenEmbedding + PositionEncoding\n",
        "\n",
        "Without it ‚Üí model cannot distinguish word order.\n"
      ],
      "metadata": {
        "id": "Wfx_iGX6xJj-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3HLRhKuxXQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}